{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa92eb05-4369-444e-b9f8-4d5f446e6147",
   "metadata": {},
   "source": [
    "Predicting Road Accident Risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6c0dabd5-2024-49f7-9230-df37b6787662",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2556ae7-9fba-4f76-b3d9-518cd4799003",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "from sklearn.impute import SimpleImputer\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "\n",
    "print('All libraries imported successfully!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f5c114-6fe5-4404-a089-9266486c123d",
   "metadata": {},
   "source": [
    "# Load Training and Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048e25d6-d243-4726-ba2b-142db2f0d3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\n",
    "test = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\n",
    "sample_submission = pd.read_csv('/kaggle/input/playground-series-s5e10/sample_submission.csv')\n",
    "\n",
    "print(f'Training Data Shape: {train.shape}')\n",
    "print(f'Test Data Shape: {test.shape}')\n",
    "print(f'\\nTraining Data Head:')\n",
    "print(train.head())\n",
    "print(f'\\nTest Data Head:')\n",
    "print(test.head())\n",
    "print(f'\\nSample Submission Head:')\n",
    "print(sample_submission.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ba0f7f-49d7-4ef1-b494-4809d5ce20bb",
   "metadata": {},
   "source": [
    "# EDA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73bc990f-6463-4d44-a10f-5cd3959fbac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed data exploration\n",
    "print('Data Info:')\n",
    "print(train.info())\n",
    "print(f'\\nDescriptive Statistics:')\n",
    "print(train.describe())\n",
    "print(f'\\nMissing Values in Train:')\n",
    "print(train.isnull().sum())\n",
    "print(f'\\nMissing Values in Test:')\n",
    "print(test.isnull().sum())\n",
    "\n",
    "# Identify column types\n",
    "categorical_cols = train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "numerical_cols = [col for col in numerical_cols if col not in ['id', 'accident_risk']]\n",
    "\n",
    "print(f'\\nCategorical Columns: {categorical_cols}')\n",
    "print(f'Numerical Columns: {numerical_cols}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6107b24-a6a9-42e7-ac21-4b2a1378dae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].hist(train['accident_risk'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "axes[0].set_title('Distribution of Accident Risk', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Accident Risk')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1].boxplot(train['accident_risk'])\n",
    "axes[1].set_title('Boxplot of Accident Risk', fontsize=12, fontweight='bold')\n",
    "axes[1].set_ylabel('Accident Risk')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f'Target Variable Statistics:')\n",
    "print(train['accident_risk'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96320c4f-6373-4d52-a74f-7d43eba57e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    \"\"\"Create advanced features for accident risk prediction\"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # ===== Interaction Features =====\n",
    "    df_copy['speed_curvature_interaction'] = df_copy['speed_limit'] * df_copy['curvature']\n",
    "    df_copy['lanes_curvature_interaction'] = df_copy['num_lanes'] * df_copy['curvature']\n",
    "    df_copy['speed_lanes_ratio'] = df_copy['speed_limit'] / (df_copy['num_lanes'] + 1)\n",
    "    df_copy['speed_num_accidents_interaction'] = df_copy['speed_limit'] * df_copy['num_reported_accidents']\n",
    "    \n",
    "    # ===== Non-linear Transformations =====\n",
    "    df_copy['curvature_squared'] = df_copy['curvature'] ** 2\n",
    "    df_copy['speed_squared'] = df_copy['speed_limit'] ** 2\n",
    "    df_copy['log_speed'] = np.log1p(df_copy['speed_limit'])\n",
    "    df_copy['log_accidents'] = np.log1p(df_copy['num_reported_accidents'])\n",
    "    \n",
    "    # ===== Accident Risk Features =====\n",
    "    df_copy['accident_per_lane'] = (df_copy['num_reported_accidents'] + 1) / (df_copy['num_lanes'] + 1)\n",
    "    df_copy['accident_rate_high'] = (df_copy['num_reported_accidents'] > df_copy['num_reported_accidents'].median()).astype(int)\n",
    "    \n",
    "    # ===== Boolean Flag Features =====\n",
    "    df_copy['has_signs'] = df_copy['road_signs_present'].astype(int)\n",
    "    df_copy['is_public'] = df_copy['public_road'].astype(int)\n",
    "    df_copy['is_holiday'] = df_copy['holiday'].astype(int)\n",
    "    df_copy['is_school_season'] = df_copy['school_season'].astype(int)\n",
    "    \n",
    "    # ===== Time-based Features =====\n",
    "    time_of_day_map = {'morning': 1, 'afternoon': 2, 'evening': 3, 'night': 4}\n",
    "    df_copy['time_numeric'] = df_copy['time_of_day'].map(time_of_day_map)\n",
    "    \n",
    "    # ===== Weather Risk Encoding =====\n",
    "    weather_risk = {'clear': 1, 'rainy': 3, 'foggy': 4, 'snowy': 4}\n",
    "    df_copy['weather_risk'] = df_copy['weather'].map(weather_risk)\n",
    "    \n",
    "    # ===== Lighting Encoding =====\n",
    "    lighting_map = {'daylight': 1, 'dim': 2, 'night': 3}\n",
    "    df_copy['lighting_numeric'] = df_copy['lighting'].map(lighting_map)\n",
    "    \n",
    "    # ===== Combined Risk Score =====\n",
    "    df_copy['combined_risk_score'] = (df_copy['curvature'] * 0.3 + \n",
    "                                       (df_copy['weather_risk'] / 4) * 0.2 + \n",
    "                                       (df_copy['lighting_numeric'] / 3) * 0.2 + \n",
    "                                       (df_copy['num_reported_accidents'] / 7) * 0.3)\n",
    "    \n",
    "    # ===== Road Type Features =====\n",
    "    road_type_map = {'urban': 1, 'rural': 2, 'highway': 3}\n",
    "    df_copy['road_type_numeric'] = df_copy['road_type'].map(road_type_map)\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "# Apply feature engineering\n",
    "train_fe = create_features(train)\n",
    "test_fe = create_features(test)\n",
    "\n",
    "print('Features created successfully!')\n",
    "print(f'Train shape after feature engineering: {train_fe.shape}')\n",
    "print(f'Test shape after feature engineering: {test_fe.shape}')\n",
    "\n",
    "new_features = [col for col in train_fe.columns if col not in train.columns]\n",
    "print(f'\\nNew features created ({len(new_features)}): {new_features}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc740fc9-a93c-4461-bf1c-8e857c2b8ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features and target\n",
    "X_train = train_fe.drop(['id', 'accident_risk'], axis=1)\n",
    "y_train = train_fe['accident_risk']\n",
    "X_test = test_fe.drop(['id'], axis=1)\n",
    "test_ids = test_fe['id'].values\n",
    "\n",
    "# Identify column types\n",
    "categorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "numerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "\n",
    "print(f'Categorical features: {categorical_features}')\n",
    "print(f'Numerical features count: {len(numerical_features)}')\n",
    "print(f'Total features: {len(X_train.columns)}')\n",
    "\n",
    "# Create preprocessing pipeline\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_transformer, numerical_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Fit preprocessor\n",
    "X_train_processed = preprocessor.fit_transform(X_train)\n",
    "X_test_processed = preprocessor.transform(X_test)\n",
    "\n",
    "print(f'\\nProcessed train shape: {X_train_processed.shape}')\n",
    "print(f'Processed test shape: {X_test_processed.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "851ccf7f-ddfa-45a9-8ac8-ae55165a4bae",
   "metadata": {},
   "source": [
    "# Model Training with Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9821f1c0-7f33-4aa7-9654-ab26cbefe1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data for validation\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "    X_train_processed, y_train, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "print('Training individual models with optimized hyperparameters...')\n",
    "print('='*60)\n",
    "\n",
    "# ===== XGBoost =====\n",
    "print('\\n1. Training XGBoost...')\n",
    "xgb_model = XGBRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=1,\n",
    "    gamma=0,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "xgb_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "xgb_pred_val = xgb_model.predict(X_val)\n",
    "xgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred_val))\n",
    "xgb_mae = mean_absolute_error(y_val, xgb_pred_val)\n",
    "xgb_r2 = r2_score(y_val, xgb_pred_val)\n",
    "print(f'   XGBoost RMSE: {xgb_rmse:.6f}, MAE: {xgb_mae:.6f}, R²: {xgb_r2:.6f}')\n",
    "\n",
    "# ===== LightGBM =====\n",
    "print('\\n2. Training LightGBM...')\n",
    "lgb_model = LGBMRegressor(\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=6,\n",
    "    num_leaves=31,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    min_child_weight=1,\n",
    "    random_state=42,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "print('\\n2. Training LightGBM...')\n",
    "lgb_model = LGBMRegressor(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31,\n",
    "    subsample=0.8, colsample_bytree=0.8, min_child_weight=1,\n",
    "    random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "lgb_model.fit(X_tr, y_tr)  # ✅ Simple fit - no callbacks!\n",
    "lgb_pred_val = lgb_model.predict(X_val)\n",
    "lgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred_val))\n",
    "lgb_mae = mean_absolute_error(y_val, lgb_pred_val)\n",
    "lgb_r2 = r2_score(y_val, lgb_pred_val)\n",
    "print(f'   LightGBM RMSE: {lgb_rmse:.6f}, MAE: {lgb_mae:.6f}, R²: {lgb_r2:.6f}')\n",
    "\n",
    "# ===== CatBoost =====\n",
    "print('\\n3. Training CatBoost...')\n",
    "cat_model = CatBoostRegressor(\n",
    "    iterations=500,\n",
    "    learning_rate=0.05,\n",
    "    depth=6,\n",
    "    subsample=0.8,\n",
    "    random_state=42,\n",
    "    verbose=False\n",
    ")\n",
    "cat_model.fit(\n",
    "    X_tr, y_tr,\n",
    "    eval_set=[(X_val, y_val)],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose=False\n",
    ")\n",
    "cat_pred_val = cat_model.predict(X_val)\n",
    "cat_rmse = np.sqrt(mean_squared_error(y_val, cat_pred_val))\n",
    "cat_mae = mean_absolute_error(y_val, cat_pred_val)\n",
    "cat_r2 = r2_score(y_val, cat_pred_val)\n",
    "print(f'   CatBoost RMSE: {cat_rmse:.6f}, MAE: {cat_mae:.6f}, R²: {cat_r2:.6f}')\n",
    "\n",
    "# ===== Random Forest =====\n",
    "print('\\n4. Training Random Forest...')\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=300,\n",
    "    max_depth=15,\n",
    "    min_samples_split=5,\n",
    "    min_samples_leaf=2,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf_model.fit(X_tr, y_tr)\n",
    "rf_pred_val = rf_model.predict(X_val)\n",
    "rf_rmse = np.sqrt(mean_squared_error(y_val, rf_pred_val))\n",
    "rf_mae = mean_absolute_error(y_val, rf_pred_val)\n",
    "rf_r2 = r2_score(y_val, rf_pred_val)\n",
    "print(f'   Random Forest RMSE: {rf_rmse:.6f}, MAE: {rf_mae:.6f}, R²: {rf_r2:.6f}')\n",
    "\n",
    "print('\\n' + '='*60)\n",
    "print('All models trained successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c703e56c-f31d-47e7-bcea-3e279061c695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform K-Fold cross-validation for more robust evaluation\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print('Performing 5-Fold Cross-Validation...')\n",
    "print('='*60)\n",
    "\n",
    "# XGBoost CV\n",
    "xgb_cv_scores = cross_val_score(\n",
    "    xgb_model, X_train_processed, y_train,\n",
    "    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "xgb_cv_rmse = np.sqrt(-xgb_cv_scores)\n",
    "print(f'\\nXGBoost CV RMSE: {xgb_cv_rmse.mean():.6f} (+/- {xgb_cv_rmse.std():.6f})')\n",
    "print(f'  Fold scores: {[f\"{x:.6f}\" for x in xgb_cv_rmse]}')\n",
    "\n",
    "# LightGBM CV\n",
    "lgb_cv_scores = cross_val_score(\n",
    "    lgb_model, X_train_processed, y_train,\n",
    "    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "lgb_cv_rmse = np.sqrt(-lgb_cv_scores)\n",
    "print(f'\\nLightGBM CV RMSE: {lgb_cv_rmse.mean():.6f} (+/- {lgb_cv_rmse.std():.6f})')\n",
    "print(f'  Fold scores: {[f\"{x:.6f}\" for x in lgb_cv_rmse]}')\n",
    "\n",
    "# CatBoost CV\n",
    "cat_cv_scores = cross_val_score(\n",
    "    cat_model, X_train_processed, y_train,\n",
    "    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "cat_cv_rmse = np.sqrt(-cat_cv_scores)\n",
    "print(f'\\nCatBoost CV RMSE: {cat_cv_rmse.mean():.6f} (+/- {cat_cv_rmse.std():.6f})')\n",
    "print(f'  Fold scores: {[f\"{x:.6f}\" for x in cat_cv_rmse]}')\n",
    "\n",
    "# Random Forest CV\n",
    "rf_cv_scores = cross_val_score(\n",
    "    rf_model, X_train_processed, y_train,\n",
    "    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n",
    ")\n",
    "rf_cv_rmse = np.sqrt(-rf_cv_scores)\n",
    "print(f'\\nRandom Forest CV RMSE: {rf_cv_rmse.mean():.6f} (+/- {rf_cv_rmse.std():.6f})')\n",
    "print(f'  Fold scores: {[f\"{x:.6f}\" for x in rf_cv_rmse]}')\n",
    "\n",
    "print('\\n' + '='*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616a98b3-32b7-47ae-81aa-cdd37f1b9542",
   "metadata": {},
   "source": [
    "# Weighted Ensemble Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7342dc44-525c-4bb0-b8a9-f6763ac9d1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create weighted ensemble based on CV performance\n",
    "print('Creating Weighted Ensemble Model...')\n",
    "print('='*60)\n",
    "\n",
    "# Retrain models on full training data\n",
    "print('\\nRetraining models on full dataset...')\n",
    "\n",
    "xgb_final = XGBRegressor(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=6,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method='hist'\n",
    ")\n",
    "xgb_final.fit(X_train_processed, y_train, verbose=False)\n",
    "\n",
    "lgb_final = LGBMRegressor(\n",
    "    n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31,\n",
    "    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1\n",
    ")\n",
    "lgb_final.fit(X_train_processed, y_train)\n",
    "\n",
    "cat_final = CatBoostRegressor(\n",
    "    iterations=500, learning_rate=0.05, depth=6, subsample=0.8, random_state=42, verbose=False\n",
    ")\n",
    "cat_final.fit(X_train_processed, y_train, verbose=False)\n",
    "\n",
    "rf_final = RandomForestRegressor(\n",
    "    n_estimators=300, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1\n",
    ")\n",
    "rf_final.fit(X_train_processed, y_train)\n",
    "\n",
    "# Generate predictions\n",
    "xgb_pred_test = xgb_final.predict(X_test_processed)\n",
    "lgb_pred_test = lgb_final.predict(X_test_processed)\n",
    "cat_pred_test = cat_final.predict(X_test_processed)\n",
    "rf_pred_test = rf_final.predict(X_test_processed)\n",
    "\n",
    "# Calculate ensemble weights (inverse of CV RMSE)\n",
    "weights = np.array([\n",
    "    xgb_cv_rmse.mean(),\n",
    "    lgb_cv_rmse.mean(),\n",
    "    cat_cv_rmse.mean(),\n",
    "    rf_cv_rmse.mean()\n",
    "])\n",
    "weights = 1 / weights\n",
    "weights = weights / weights.sum()\n",
    "\n",
    "print(f'\\nEnsemble Weights:')\n",
    "print(f'  XGBoost:      {weights[0]:.4f} ({weights[0]*100:.2f}%)')\n",
    "print(f'  LightGBM:     {weights[1]:.4f} ({weights[1]*100:.2f}%)')\n",
    "print(f'  CatBoost:     {weights[2]:.4f} ({weights[2]*100:.2f}%)')\n",
    "print(f'  Random Forest: {weights[3]:.4f} ({weights[3]*100:.2f}%)')\n",
    "\n",
    "# Create ensemble predictions\n",
    "ensemble_pred = (\n",
    "    weights[0] * xgb_pred_test +\n",
    "    weights[1] * lgb_pred_test +\n",
    "    weights[2] * cat_pred_test +\n",
    "    weights[3] * rf_pred_test\n",
    ")\n",
    "\n",
    "print(f'\\nEnsemble predictions shape: {ensemble_pred.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c222fad8-6037-4cf5-906d-240b05b80e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nCreating submission...')\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'accident_risk': ensemble_pred\n",
    "})\n",
    "\n",
    "# Clip to [0, 1] range\n",
    "submission['accident_risk'] = submission['accident_risk'].clip(0, 1)\n",
    "\n",
    "submission_path = '/kaggle/working/submission.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "print(f' Submission saved to {submission_path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60206803-9385-4d36-a4d5-1e705afb6296",
   "metadata": {},
   "source": [
    "# Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97bbdbe-d2ee-49b9-8ada-b05e56891963",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive visualization\n",
    "fig = plt.figure(figsize=(16, 12))\n",
    "gs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n",
    "\n",
    "# 1. Model Comparison - RMSE\n",
    "ax1 = fig.add_subplot(gs[0, 0])\n",
    "models = ['XGBoost', 'LightGBM', 'CatBoost', 'Random Forest']\n",
    "cv_rmse = [xgb_cv_rmse.mean(), lgb_cv_rmse.mean(), cat_cv_rmse.mean(), rf_cv_rmse.mean()]\n",
    "colors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\n",
    "ax1.bar(models, cv_rmse, color=colors, alpha=0.8, edgecolor='black')\n",
    "ax1.set_title('Cross-Validation RMSE Comparison', fontsize=11, fontweight='bold')\n",
    "ax1.set_ylabel('RMSE')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "for i, v in enumerate(cv_rmse):\n",
    "    ax1.text(i, v + 0.001, f'{v:.4f}', ha='center', fontsize=9)\n",
    "\n",
    "# 2. Ensemble Weights\n",
    "ax2 = fig.add_subplot(gs[0, 1])\n",
    "ax2.pie(weights, labels=models, autopct='%1.1f%%', colors=colors, startangle=90)\n",
    "ax2.set_title('Ensemble Model Weights', fontsize=11, fontweight='bold')\n",
    "\n",
    "# 3. Distribution of Test Predictions\n",
    "ax3 = fig.add_subplot(gs[0, 2])\n",
    "ax3.hist(ensemble_pred, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\n",
    "ax3.set_title('Distribution of Ensemble Predictions', fontsize=11, fontweight='bold')\n",
    "ax3.set_xlabel('Accident Risk')\n",
    "ax3.set_ylabel('Frequency')\n",
    "\n",
    "# 4. Validation Set Performance - XGBoost\n",
    "ax4 = fig.add_subplot(gs[1, 0])\n",
    "ax4.scatter(y_val, xgb_pred_val, alpha=0.5, s=10)\n",
    "ax4.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "ax4.set_title(f'XGBoost: Val RMSE={xgb_rmse:.4f}', fontsize=11, fontweight='bold')\n",
    "ax4.set_xlabel('Actual')\n",
    "ax4.set_ylabel('Predicted')\n",
    "\n",
    "# 5. Validation Set Performance - LightGBM\n",
    "ax5 = fig.add_subplot(gs[1, 1])\n",
    "ax5.scatter(y_val, lgb_pred_val, alpha=0.5, s=10, color='orange')\n",
    "ax5.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "ax5.set_title(f'LightGBM: Val RMSE={lgb_rmse:.4f}', fontsize=11, fontweight='bold')\n",
    "ax5.set_xlabel('Actual')\n",
    "ax5.set_ylabel('Predicted')\n",
    "\n",
    "# 6. Validation Set Performance - Ensemble\n",
    "ax6 = fig.add_subplot(gs[1, 2])\n",
    "ensemble_val = (weights[0] * xgb_pred_val + weights[1] * lgb_pred_val +\n",
    "                 weights[2] * cat_pred_val + weights[3] * rf_pred_val)\n",
    "ensemble_rmse_val = np.sqrt(mean_squared_error(y_val, ensemble_val))\n",
    "ax6.scatter(y_val, ensemble_val, alpha=0.5, s=10, color='green')\n",
    "ax6.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\n",
    "ax6.set_title(f'Ensemble: Val RMSE={ensemble_rmse_val:.4f}', fontsize=11, fontweight='bold')\n",
    "ax6.set_xlabel('Actual')\n",
    "ax6.set_ylabel('Predicted')\n",
    "\n",
    "# 7. Cross-Validation Score Distribution\n",
    "ax7 = fig.add_subplot(gs[2, 0])\n",
    "ax7.boxplot([xgb_cv_rmse, lgb_cv_rmse, cat_cv_rmse, rf_cv_rmse],\n",
    "             labels=models)\n",
    "ax7.set_title('Cross-Validation RMSE Distribution', fontsize=11, fontweight='bold')\n",
    "ax7.set_ylabel('RMSE')\n",
    "ax7.tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 8. Model Predictions Comparison\n",
    "ax8 = fig.add_subplot(gs[2, 1])\n",
    "ax8.hist(xgb_pred_test, bins=40, alpha=0.5, label='XGBoost', edgecolor='black')\n",
    "ax8.hist(lgb_pred_test, bins=40, alpha=0.5, label='LightGBM', edgecolor='black')\n",
    "ax8.hist(ensemble_pred, bins=40, alpha=0.5, label='Ensemble', edgecolor='black')\n",
    "ax8.set_title('Prediction Distribution Comparison', fontsize=11, fontweight='bold')\n",
    "ax8.set_xlabel('Accident Risk')\n",
    "ax8.set_ylabel('Frequency')\n",
    "ax8.legend()\n",
    "\n",
    "# 9. Summary Statistics Table (as text)\n",
    "ax9 = fig.add_subplot(gs[2, 2])\n",
    "ax9.axis('off')\n",
    "summary_text = f\"\"\"SUMMARY STATISTICS\n",
    "\n",
    "Validation RMSE:\n",
    "  XGB: {xgb_rmse:.6f}\n",
    "  LGB: {lgb_rmse:.6f}\n",
    "  CAT: {cat_rmse:.6f}\n",
    "  RF:  {rf_rmse:.6f}\n",
    "\n",
    "CV RMSE (Mean±Std):\n",
    "  XGB: {xgb_cv_rmse.mean():.4f}±{xgb_cv_rmse.std():.4f}\n",
    "  LGB: {lgb_cv_rmse.mean():.4f}±{lgb_cv_rmse.std():.4f}\n",
    "\n",
    "Ensemble R²: {r2_score(y_val, ensemble_val):.4f}\n",
    "\"\"\"\n",
    "ax9.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n",
    "         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "plt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Performance visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6395f2e7-3783-449b-90ef-32ba8ee4efa3",
   "metadata": {},
   "source": [
    "# Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3098671-d08b-4b1b-ac23-12f297a69936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Analysis - CORRECTED VERSION\n",
    "\n",
    "# Get feature names from the original data\n",
    "feature_names = []\n",
    "\n",
    "# Add numerical feature names\n",
    "numerical_features_list = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "feature_names.extend(numerical_features_list)\n",
    "\n",
    "# Add categorical feature names\n",
    "categorical_features_list = X_train.select_dtypes(include=['object']).columns.tolist()\n",
    "feature_names.extend(categorical_features_list)\n",
    "\n",
    "print(f'Total features in X_train: {len(X_train.columns)}')\n",
    "print(f'Feature names created: {len(feature_names)}')\n",
    "\n",
    "# Now let's get feature importances\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# ===== XGBoost Feature Importance =====\n",
    "try:\n",
    "    xgb_importances = xgb_final.feature_importances_\n",
    "    if len(xgb_importances) == len(feature_names):\n",
    "        xgb_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': xgb_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    else:\n",
    "        # If lengths don't match, create generic feature names\n",
    "        xgb_imp = pd.DataFrame({\n",
    "            'feature': [f'Feature_{i}' for i in range(len(xgb_importances))],\n",
    "            'importance': xgb_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[0, 0].barh(range(len(xgb_imp)), xgb_imp['importance'], color='#FF6B6B')\n",
    "    axes[0, 0].set_yticks(range(len(xgb_imp)))\n",
    "    axes[0, 0].set_yticklabels(xgb_imp['feature'])\n",
    "    axes[0, 0].set_title('XGBoost Feature Importance', fontsize=12, fontweight='bold')\n",
    "    axes[0, 0].invert_yaxis()\n",
    "except Exception as e:\n",
    "    print(f\"Error with XGBoost: {e}\")\n",
    "\n",
    "# ===== LightGBM Feature Importance =====\n",
    "try:\n",
    "    lgb_importances = lgb_final.feature_importances_\n",
    "    if len(lgb_importances) == len(feature_names):\n",
    "        lgb_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': lgb_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    else:\n",
    "        lgb_imp = pd.DataFrame({\n",
    "            'feature': [f'Feature_{i}' for i in range(len(lgb_importances))],\n",
    "            'importance': lgb_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[0, 1].barh(range(len(lgb_imp)), lgb_imp['importance'], color='#4ECDC4')\n",
    "    axes[0, 1].set_yticks(range(len(lgb_imp)))\n",
    "    axes[0, 1].set_yticklabels(lgb_imp['feature'])\n",
    "    axes[0, 1].set_title('LightGBM Feature Importance', fontsize=12, fontweight='bold')\n",
    "    axes[0, 1].invert_yaxis()\n",
    "except Exception as e:\n",
    "    print(f\"Error with LightGBM: {e}\")\n",
    "\n",
    "# ===== CatBoost Feature Importance =====\n",
    "try:\n",
    "    cat_importances = cat_final.get_feature_importance()\n",
    "    if len(cat_importances) == len(feature_names):\n",
    "        cat_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': cat_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    else:\n",
    "        cat_imp = pd.DataFrame({\n",
    "            'feature': [f'Feature_{i}' for i in range(len(cat_importances))],\n",
    "            'importance': cat_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[1, 0].barh(range(len(cat_imp)), cat_imp['importance'], color='#45B7D1')\n",
    "    axes[1, 0].set_yticks(range(len(cat_imp)))\n",
    "    axes[1, 0].set_yticklabels(cat_imp['feature'])\n",
    "    axes[1, 0].set_title('CatBoost Feature Importance', fontsize=12, fontweight='bold')\n",
    "    axes[1, 0].invert_yaxis()\n",
    "except Exception as e:\n",
    "    print(f\"Error with CatBoost: {e}\")\n",
    "\n",
    "# ===== Random Forest Feature Importance =====\n",
    "try:\n",
    "    rf_importances = rf_final.feature_importances_\n",
    "    if len(rf_importances) == len(feature_names):\n",
    "        rf_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': rf_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    else:\n",
    "        rf_imp = pd.DataFrame({\n",
    "            'feature': [f'Feature_{i}' for i in range(len(rf_importances))],\n",
    "            'importance': rf_importances\n",
    "        }).sort_values('importance', ascending=False).head(15)\n",
    "    \n",
    "    axes[1, 1].barh(range(len(rf_imp)), rf_imp['importance'], color='#96CEB4')\n",
    "    axes[1, 1].set_yticks(range(len(rf_imp)))\n",
    "    axes[1, 1].set_yticklabels(rf_imp['feature'])\n",
    "    axes[1, 1].set_title('Random Forest Feature Importance', fontsize=12, fontweight='bold')\n",
    "    axes[1, 1].invert_yaxis()\n",
    "except Exception as e:\n",
    "    print(f\"Error with Random Forest: {e}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Feature importance visualization saved!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a732f4a4-6e2f-499a-8297-b9b77977d86a",
   "metadata": {},
   "source": [
    "# Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8c0f80-8b95-4c1d-b467-f4d082cabdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n' + '='*70)\n",
    "print('FINAL MODEL PERFORMANCE SUMMARY'.center(70))\n",
    "print('='*70)\n",
    "\n",
    "print('\\n1. INDIVIDUAL MODEL PERFORMANCE:')\n",
    "print('-'*70)\n",
    "print(f'\\nValidation Set Metrics:')\n",
    "print(f'  Model           RMSE      MAE       R² Score')\n",
    "print(f'  {\"─\"*60}')\n",
    "print(f'  XGBoost:        {xgb_rmse:.6f}  {xgb_mae:.6f}  {xgb_r2:.6f}')\n",
    "print(f'  LightGBM:       {lgb_rmse:.6f}  {lgb_mae:.6f}  {lgb_r2:.6f}')\n",
    "print(f'  CatBoost:       {cat_rmse:.6f}  {cat_mae:.6f}  {cat_r2:.6f}')\n",
    "print(f'  Random Forest:  {rf_rmse:.6f}  {rf_mae:.6f}  {rf_r2:.6f}')\n",
    "\n",
    "print('\\n\\n2. CROSS-VALIDATION PERFORMANCE (5-Fold):')\n",
    "print('-'*70)\n",
    "print(f'  Model           Mean RMSE    Std Dev     Min         Max')\n",
    "print(f'  {\"─\"*60}')\n",
    "print(f'  XGBoost:        {xgb_cv_rmse.mean():.6f}      {xgb_cv_rmse.std():.6f}      {xgb_cv_rmse.min():.6f}      {xgb_cv_rmse.max():.6f}')\n",
    "print(f'  LightGBM:       {lgb_cv_rmse.mean():.6f}      {lgb_cv_rmse.std():.6f}      {lgb_cv_rmse.min():.6f}      {lgb_cv_rmse.max():.6f}')\n",
    "print(f'  CatBoost:       {cat_cv_rmse.mean():.6f}      {cat_cv_rmse.std():.6f}      {cat_cv_rmse.min():.6f}      {cat_cv_rmse.max():.6f}')\n",
    "print(f'  Random Forest:  {rf_cv_rmse.mean():.6f}      {rf_cv_rmse.std():.6f}      {rf_cv_rmse.min():.6f}      {rf_cv_rmse.max():.6f}')\n",
    "\n",
    "print('\\n\\n3. ENSEMBLE MODEL CONFIGURATION:')\n",
    "print('-'*70)\n",
    "print(f'  Model           Weight      Percentage')\n",
    "print(f'  {\"─\"*60}')\n",
    "print(f'  XGBoost:        {weights[0]:.6f}      {weights[0]*100:.2f}%')\n",
    "print(f'  LightGBM:       {weights[1]:.6f}      {weights[1]*100:.2f}%')\n",
    "print(f'  CatBoost:       {weights[2]:.6f}      {weights[2]*100:.2f}%')\n",
    "print(f'  Random Forest:  {weights[3]:.6f}      {weights[3]*100:.2f}%')\n",
    "\n",
    "print('\\n\\n4. TEST SET PREDICTIONS SUMMARY:')\n",
    "print('-'*70)\n",
    "print(f'  Statistic                   Value')\n",
    "print(f'  {\"─\"*60}')\n",
    "print(f'  Number of predictions:     {len(ensemble_pred):>10}')\n",
    "print(f'  Mean prediction:           {ensemble_pred.mean():>10.6f}')\n",
    "print(f'  Std deviation:             {ensemble_pred.std():>10.6f}')\n",
    "print(f'  Min prediction:            {ensemble_pred.min():>10.6f}')\n",
    "print(f'  Max prediction:            {ensemble_pred.max():>10.6f}')\n",
    "print(f'  Median prediction:         {np.median(ensemble_pred):>10.6f}')\n",
    "\n",
    "print('\\n\\n5. OUTPUT FILES GENERATED:')\n",
    "print('-'*70)\n",
    "print('  ✓ submission.csv')\n",
    "print('  ✓ feature_importance.png')\n",
    "\n",
    "print('\\n' + '='*70)\n",
    "print('ANALYSIS COMPLETE'.center(70))\n",
    "print('='*70 + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b484d059-f99d-4e59-9b34-82c34e03b570",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1ec4a4-0dd1-44cb-92af-f6c5a208a47a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
