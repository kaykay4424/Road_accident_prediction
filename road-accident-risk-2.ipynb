{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91721,"databundleVersionId":13760552,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:39.751179Z","iopub.execute_input":"2025-10-31T21:17:39.751469Z","iopub.status.idle":"2025-10-31T21:17:39.765126Z","shell.execute_reply.started":"2025-10-31T21:17:39.75145Z","shell.execute_reply":"2025-10-31T21:17:39.764078Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, StratifiedKFold\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\nfrom sklearn.impute import SimpleImputer\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom catboost import CatBoostRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n\nprint('All libraries imported successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:39.766644Z","iopub.execute_input":"2025-10-31T21:17:39.76701Z","iopub.status.idle":"2025-10-31T21:17:39.776108Z","shell.execute_reply.started":"2025-10-31T21:17:39.766981Z","shell.execute_reply":"2025-10-31T21:17:39.775153Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Load Training and Test Data****","metadata":{}},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv('/kaggle/input/playground-series-s5e10/train.csv')\ntest = pd.read_csv('/kaggle/input/playground-series-s5e10/test.csv')\nsample_submission = pd.read_csv('/kaggle/input/playground-series-s5e10/sample_submission.csv')\n\nprint(f'Training Data Shape: {train.shape}')\nprint(f'Test Data Shape: {test.shape}')\nprint(f'\\nTraining Data Head:')\nprint(train.head())\nprint(f'\\nTest Data Head:')\nprint(test.head())\nprint(f'\\nSample Submission Head:')\nprint(sample_submission.head())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:39.776973Z","iopub.execute_input":"2025-10-31T21:17:39.777228Z","iopub.status.idle":"2025-10-31T21:17:41.082917Z","shell.execute_reply.started":"2025-10-31T21:17:39.777208Z","shell.execute_reply":"2025-10-31T21:17:41.08194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Data Exploration and profiling****","metadata":{}},{"cell_type":"code","source":"# Detailed data exploration\nprint('Data Info:')\nprint(train.info())\nprint(f'\\nDescriptive Statistics:')\nprint(train.describe())\nprint(f'\\nMissing Values in Train:')\nprint(train.isnull().sum())\nprint(f'\\nMissing Values in Test:')\nprint(test.isnull().sum())\n\n# Identify column types\ncategorical_cols = train.select_dtypes(include=['object']).columns.tolist()\nnumerical_cols = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nnumerical_cols = [col for col in numerical_cols if col not in ['id', 'accident_risk']]\n\nprint(f'\\nCategorical Columns: {categorical_cols}')\nprint(f'Numerical Columns: {numerical_cols}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:41.084869Z","iopub.execute_input":"2025-10-31T21:17:41.085298Z","iopub.status.idle":"2025-10-31T21:17:41.493826Z","shell.execute_reply.started":"2025-10-31T21:17:41.085258Z","shell.execute_reply":"2025-10-31T21:17:41.492792Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Visualize target variable distribution\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\naxes[0].hist(train['accident_risk'], bins=50, edgecolor='black', alpha=0.7, color='steelblue')\naxes[0].set_title('Distribution of Accident Risk', fontsize=12, fontweight='bold')\naxes[0].set_xlabel('Accident Risk')\naxes[0].set_ylabel('Frequency')\n\naxes[1].boxplot(train['accident_risk'])\naxes[1].set_title('Boxplot of Accident Risk', fontsize=12, fontweight='bold')\naxes[1].set_ylabel('Accident Risk')\n\nplt.tight_layout()\nplt.show()\n\nprint(f'Target Variable Statistics:')\nprint(train['accident_risk'].describe())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:41.494779Z","iopub.execute_input":"2025-10-31T21:17:41.495106Z","iopub.status.idle":"2025-10-31T21:17:41.942936Z","shell.execute_reply.started":"2025-10-31T21:17:41.495075Z","shell.execute_reply":"2025-10-31T21:17:41.941918Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Advance Feature Engineering****","metadata":{}},{"cell_type":"code","source":"def create_features(df):\n    \"\"\"Create advanced features for accident risk prediction\"\"\"\n    df_copy = df.copy()\n    \n    # ===== Interaction Features =====\n    df_copy['speed_curvature_interaction'] = df_copy['speed_limit'] * df_copy['curvature']\n    df_copy['lanes_curvature_interaction'] = df_copy['num_lanes'] * df_copy['curvature']\n    df_copy['speed_lanes_ratio'] = df_copy['speed_limit'] / (df_copy['num_lanes'] + 1)\n    df_copy['speed_num_accidents_interaction'] = df_copy['speed_limit'] * df_copy['num_reported_accidents']\n    \n    # ===== Non-linear Transformations =====\n    df_copy['curvature_squared'] = df_copy['curvature'] ** 2\n    df_copy['speed_squared'] = df_copy['speed_limit'] ** 2\n    df_copy['log_speed'] = np.log1p(df_copy['speed_limit'])\n    df_copy['log_accidents'] = np.log1p(df_copy['num_reported_accidents'])\n    \n    # ===== Accident Risk Features =====\n    df_copy['accident_per_lane'] = (df_copy['num_reported_accidents'] + 1) / (df_copy['num_lanes'] + 1)\n    df_copy['accident_rate_high'] = (df_copy['num_reported_accidents'] > df_copy['num_reported_accidents'].median()).astype(int)\n    \n    # ===== Boolean Flag Features =====\n    df_copy['has_signs'] = df_copy['road_signs_present'].astype(int)\n    df_copy['is_public'] = df_copy['public_road'].astype(int)\n    df_copy['is_holiday'] = df_copy['holiday'].astype(int)\n    df_copy['is_school_season'] = df_copy['school_season'].astype(int)\n    \n    # ===== Time-based Features =====\n    time_of_day_map = {'morning': 1, 'afternoon': 2, 'evening': 3, 'night': 4}\n    df_copy['time_numeric'] = df_copy['time_of_day'].map(time_of_day_map)\n    \n    # ===== Weather Risk Encoding =====\n    weather_risk = {'clear': 1, 'rainy': 3, 'foggy': 4, 'snowy': 4}\n    df_copy['weather_risk'] = df_copy['weather'].map(weather_risk)\n    \n    # ===== Lighting Encoding =====\n    lighting_map = {'daylight': 1, 'dim': 2, 'night': 3}\n    df_copy['lighting_numeric'] = df_copy['lighting'].map(lighting_map)\n    \n    # ===== Combined Risk Score =====\n    df_copy['combined_risk_score'] = (df_copy['curvature'] * 0.3 + \n                                       (df_copy['weather_risk'] / 4) * 0.2 + \n                                       (df_copy['lighting_numeric'] / 3) * 0.2 + \n                                       (df_copy['num_reported_accidents'] / 7) * 0.3)\n    \n    # ===== Road Type Features =====\n    road_type_map = {'urban': 1, 'rural': 2, 'highway': 3}\n    df_copy['road_type_numeric'] = df_copy['road_type'].map(road_type_map)\n    \n    return df_copy\n\n# Apply feature engineering\ntrain_fe = create_features(train)\ntest_fe = create_features(test)\n\nprint('Features created successfully!')\nprint(f'Train shape after feature engineering: {train_fe.shape}')\nprint(f'Test shape after feature engineering: {test_fe.shape}')\n\nnew_features = [col for col in train_fe.columns if col not in train.columns]\nprint(f'\\nNew features created ({len(new_features)}): {new_features}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:41.944132Z","iopub.execute_input":"2025-10-31T21:17:41.944465Z","iopub.status.idle":"2025-10-31T21:17:42.376227Z","shell.execute_reply.started":"2025-10-31T21:17:41.944434Z","shell.execute_reply":"2025-10-31T21:17:42.375365Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Build Preprocessing Pipeline****","metadata":{}},{"cell_type":"code","source":"# Prepare features and target\nX_train = train_fe.drop(['id', 'accident_risk'], axis=1)\ny_train = train_fe['accident_risk']\nX_test = test_fe.drop(['id'], axis=1)\ntest_ids = test_fe['id'].values\n\n# Identify column types\ncategorical_features = X_train.select_dtypes(include=['object']).columns.tolist()\nnumerical_features = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n\nprint(f'Categorical features: {categorical_features}')\nprint(f'Numerical features count: {len(numerical_features)}')\nprint(f'Total features: {len(X_train.columns)}')\n\n# Create preprocessing pipeline\nnumerical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('ordinal', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1))\n])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Fit preprocessor\nX_train_processed = preprocessor.fit_transform(X_train)\nX_test_processed = preprocessor.transform(X_test)\n\nprint(f'\\nProcessed train shape: {X_train_processed.shape}')\nprint(f'Processed test shape: {X_test_processed.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:42.377118Z","iopub.execute_input":"2025-10-31T21:17:42.377357Z","iopub.status.idle":"2025-10-31T21:17:45.867184Z","shell.execute_reply.started":"2025-10-31T21:17:42.377337Z","shell.execute_reply":"2025-10-31T21:17:45.866221Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Training with Cross Validation","metadata":{}},{"cell_type":"code","source":"# Split data for validation\nX_tr, X_val, y_tr, y_val = train_test_split(\n    X_train_processed, y_train, test_size=0.2, random_state=42\n)\n\nprint('Training individual models with optimized hyperparameters...')\nprint('='*60)\n\n# ===== XGBoost =====\nprint('\\n1. Training XGBoost...')\nxgb_model = XGBRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    min_child_weight=1,\n    gamma=0,\n    random_state=42,\n    n_jobs=-1,\n    tree_method='hist'\n)\nxgb_model.fit(\n    X_tr, y_tr,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=50,\n    verbose=False\n)\nxgb_pred_val = xgb_model.predict(X_val)\nxgb_rmse = np.sqrt(mean_squared_error(y_val, xgb_pred_val))\nxgb_mae = mean_absolute_error(y_val, xgb_pred_val)\nxgb_r2 = r2_score(y_val, xgb_pred_val)\nprint(f'   XGBoost RMSE: {xgb_rmse:.6f}, MAE: {xgb_mae:.6f}, R²: {xgb_r2:.6f}')\n\n# ===== LightGBM =====\nprint('\\n2. Training LightGBM...')\nlgb_model = LGBMRegressor(\n    n_estimators=500,\n    learning_rate=0.05,\n    max_depth=6,\n    num_leaves=31,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    min_child_weight=1,\n    random_state=42,\n    n_jobs=-1,\n    verbose=-1\n)\nprint('\\n2. Training LightGBM...')\nlgb_model = LGBMRegressor(\n    n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31,\n    subsample=0.8, colsample_bytree=0.8, min_child_weight=1,\n    random_state=42, n_jobs=-1, verbose=-1\n)\nlgb_model.fit(X_tr, y_tr)  # ✅ Simple fit - no callbacks!\nlgb_pred_val = lgb_model.predict(X_val)\nlgb_rmse = np.sqrt(mean_squared_error(y_val, lgb_pred_val))\nlgb_mae = mean_absolute_error(y_val, lgb_pred_val)\nlgb_r2 = r2_score(y_val, lgb_pred_val)\nprint(f'   LightGBM RMSE: {lgb_rmse:.6f}, MAE: {lgb_mae:.6f}, R²: {lgb_r2:.6f}')\n\n# ===== CatBoost =====\nprint('\\n3. Training CatBoost...')\ncat_model = CatBoostRegressor(\n    iterations=500,\n    learning_rate=0.05,\n    depth=6,\n    subsample=0.8,\n    random_state=42,\n    verbose=False\n)\ncat_model.fit(\n    X_tr, y_tr,\n    eval_set=[(X_val, y_val)],\n    early_stopping_rounds=50,\n    verbose=False\n)\ncat_pred_val = cat_model.predict(X_val)\ncat_rmse = np.sqrt(mean_squared_error(y_val, cat_pred_val))\ncat_mae = mean_absolute_error(y_val, cat_pred_val)\ncat_r2 = r2_score(y_val, cat_pred_val)\nprint(f'   CatBoost RMSE: {cat_rmse:.6f}, MAE: {cat_mae:.6f}, R²: {cat_r2:.6f}')\n\n# ===== Random Forest =====\nprint('\\n4. Training Random Forest...')\nrf_model = RandomForestRegressor(\n    n_estimators=300,\n    max_depth=15,\n    min_samples_split=5,\n    min_samples_leaf=2,\n    random_state=42,\n    n_jobs=-1\n)\nrf_model.fit(X_tr, y_tr)\nrf_pred_val = rf_model.predict(X_val)\nrf_rmse = np.sqrt(mean_squared_error(y_val, rf_pred_val))\nrf_mae = mean_absolute_error(y_val, rf_pred_val)\nrf_r2 = r2_score(y_val, rf_pred_val)\nprint(f'   Random Forest RMSE: {rf_rmse:.6f}, MAE: {rf_mae:.6f}, R²: {rf_r2:.6f}')\n\nprint('\\n' + '='*60)\nprint('All models trained successfully!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:17:45.868115Z","iopub.execute_input":"2025-10-31T21:17:45.868355Z","iopub.status.idle":"2025-10-31T21:24:05.774695Z","shell.execute_reply.started":"2025-10-31T21:17:45.868335Z","shell.execute_reply":"2025-10-31T21:24:05.77379Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"K-Fold Cross Validation","metadata":{}},{"cell_type":"code","source":"# Perform K-Fold cross-validation for more robust evaluation\nkfold = KFold(n_splits=5, shuffle=True, random_state=42)\n\nprint('Performing 5-Fold Cross-Validation...')\nprint('='*60)\n\n# XGBoost CV\nxgb_cv_scores = cross_val_score(\n    xgb_model, X_train_processed, y_train,\n    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n)\nxgb_cv_rmse = np.sqrt(-xgb_cv_scores)\nprint(f'\\nXGBoost CV RMSE: {xgb_cv_rmse.mean():.6f} (+/- {xgb_cv_rmse.std():.6f})')\nprint(f'  Fold scores: {[f\"{x:.6f}\" for x in xgb_cv_rmse]}')\n\n# LightGBM CV\nlgb_cv_scores = cross_val_score(\n    lgb_model, X_train_processed, y_train,\n    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n)\nlgb_cv_rmse = np.sqrt(-lgb_cv_scores)\nprint(f'\\nLightGBM CV RMSE: {lgb_cv_rmse.mean():.6f} (+/- {lgb_cv_rmse.std():.6f})')\nprint(f'  Fold scores: {[f\"{x:.6f}\" for x in lgb_cv_rmse]}')\n\n# CatBoost CV\ncat_cv_scores = cross_val_score(\n    cat_model, X_train_processed, y_train,\n    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n)\ncat_cv_rmse = np.sqrt(-cat_cv_scores)\nprint(f'\\nCatBoost CV RMSE: {cat_cv_rmse.mean():.6f} (+/- {cat_cv_rmse.std():.6f})')\nprint(f'  Fold scores: {[f\"{x:.6f}\" for x in cat_cv_rmse]}')\n\n# Random Forest CV\nrf_cv_scores = cross_val_score(\n    rf_model, X_train_processed, y_train,\n    cv=kfold, scoring='neg_mean_squared_error', n_jobs=-1\n)\nrf_cv_rmse = np.sqrt(-rf_cv_scores)\nprint(f'\\nRandom Forest CV RMSE: {rf_cv_rmse.mean():.6f} (+/- {rf_cv_rmse.std():.6f})')\nprint(f'  Fold scores: {[f\"{x:.6f}\" for x in rf_cv_rmse]}')\n\nprint('\\n' + '='*60)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T21:24:05.777466Z","iopub.execute_input":"2025-10-31T21:24:05.777796Z","iopub.status.idle":"2025-10-31T22:11:58.375754Z","shell.execute_reply.started":"2025-10-31T21:24:05.777775Z","shell.execute_reply":"2025-10-31T22:11:58.37419Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Weighted Ensemble Model","metadata":{}},{"cell_type":"code","source":"# Create weighted ensemble based on CV performance\nprint('Creating Weighted Ensemble Model...')\nprint('='*60)\n\n# Retrain models on full training data\nprint('\\nRetraining models on full dataset...')\n\nxgb_final = XGBRegressor(\n    n_estimators=500, learning_rate=0.05, max_depth=6,\n    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, tree_method='hist'\n)\nxgb_final.fit(X_train_processed, y_train, verbose=False)\n\nlgb_final = LGBMRegressor(\n    n_estimators=500, learning_rate=0.05, max_depth=6, num_leaves=31,\n    subsample=0.8, colsample_bytree=0.8, random_state=42, n_jobs=-1, verbose=-1\n)\nlgb_final.fit(X_train_processed, y_train)\n\ncat_final = CatBoostRegressor(\n    iterations=500, learning_rate=0.05, depth=6, subsample=0.8, random_state=42, verbose=False\n)\ncat_final.fit(X_train_processed, y_train, verbose=False)\n\nrf_final = RandomForestRegressor(\n    n_estimators=300, max_depth=15, min_samples_split=5, min_samples_leaf=2, random_state=42, n_jobs=-1\n)\nrf_final.fit(X_train_processed, y_train)\n\n# Generate predictions\nxgb_pred_test = xgb_final.predict(X_test_processed)\nlgb_pred_test = lgb_final.predict(X_test_processed)\ncat_pred_test = cat_final.predict(X_test_processed)\nrf_pred_test = rf_final.predict(X_test_processed)\n\n# Calculate ensemble weights (inverse of CV RMSE)\nweights = np.array([\n    xgb_cv_rmse.mean(),\n    lgb_cv_rmse.mean(),\n    cat_cv_rmse.mean(),\n    rf_cv_rmse.mean()\n])\nweights = 1 / weights\nweights = weights / weights.sum()\n\nprint(f'\\nEnsemble Weights:')\nprint(f'  XGBoost:      {weights[0]:.4f} ({weights[0]*100:.2f}%)')\nprint(f'  LightGBM:     {weights[1]:.4f} ({weights[1]*100:.2f}%)')\nprint(f'  CatBoost:     {weights[2]:.4f} ({weights[2]*100:.2f}%)')\nprint(f'  Random Forest: {weights[3]:.4f} ({weights[3]*100:.2f}%)')\n\n# Create ensemble predictions\nensemble_pred = (\n    weights[0] * xgb_pred_test +\n    weights[1] * lgb_pred_test +\n    weights[2] * cat_pred_test +\n    weights[3] * rf_pred_test\n)\n\nprint(f'\\nEnsemble predictions shape: {ensemble_pred.shape}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T22:11:58.377411Z","iopub.execute_input":"2025-10-31T22:11:58.377714Z","iopub.status.idle":"2025-10-31T22:20:15.292001Z","shell.execute_reply.started":"2025-10-31T22:11:58.377692Z","shell.execute_reply":"2025-10-31T22:20:15.290988Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate Submission","metadata":{}},{"cell_type":"code","source":"# CRITICAL: Create submission in correct format\nprint('\\nCreating submission...')\n\nsubmission = pd.DataFrame({\n    'id': test_ids,\n    'accident_risk': ensemble_pred\n})\n\n# Clip to [0, 1] range\nsubmission['accident_risk'] = submission['accident_risk'].clip(0, 1)\n\n# ✅ CORRECT PATH for Kaggle!\nsubmission_path = '/kaggle/working/submission.csv'\nsubmission.to_csv(submission_path, index=False)\nprint(f'✓ Submission saved to {submission_path}')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T22:55:27.782599Z","iopub.execute_input":"2025-10-31T22:55:27.783226Z","iopub.status.idle":"2025-10-31T22:55:28.245435Z","shell.execute_reply.started":"2025-10-31T22:55:27.783197Z","shell.execute_reply":"2025-10-31T22:55:28.244572Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Model Performance Visualization","metadata":{}},{"cell_type":"code","source":"# Comprehensive visualization\nfig = plt.figure(figsize=(16, 12))\ngs = fig.add_gridspec(3, 3, hspace=0.35, wspace=0.3)\n\n# 1. Model Comparison - RMSE\nax1 = fig.add_subplot(gs[0, 0])\nmodels = ['XGBoost', 'LightGBM', 'CatBoost', 'Random Forest']\ncv_rmse = [xgb_cv_rmse.mean(), lgb_cv_rmse.mean(), cat_cv_rmse.mean(), rf_cv_rmse.mean()]\ncolors = ['#FF6B6B', '#4ECDC4', '#45B7D1', '#96CEB4']\nax1.bar(models, cv_rmse, color=colors, alpha=0.8, edgecolor='black')\nax1.set_title('Cross-Validation RMSE Comparison', fontsize=11, fontweight='bold')\nax1.set_ylabel('RMSE')\nax1.tick_params(axis='x', rotation=45)\nfor i, v in enumerate(cv_rmse):\n    ax1.text(i, v + 0.001, f'{v:.4f}', ha='center', fontsize=9)\n\n# 2. Ensemble Weights\nax2 = fig.add_subplot(gs[0, 1])\nax2.pie(weights, labels=models, autopct='%1.1f%%', colors=colors, startangle=90)\nax2.set_title('Ensemble Model Weights', fontsize=11, fontweight='bold')\n\n# 3. Distribution of Test Predictions\nax3 = fig.add_subplot(gs[0, 2])\nax3.hist(ensemble_pred, bins=50, color='steelblue', alpha=0.7, edgecolor='black')\nax3.set_title('Distribution of Ensemble Predictions', fontsize=11, fontweight='bold')\nax3.set_xlabel('Accident Risk')\nax3.set_ylabel('Frequency')\n\n# 4. Validation Set Performance - XGBoost\nax4 = fig.add_subplot(gs[1, 0])\nax4.scatter(y_val, xgb_pred_val, alpha=0.5, s=10)\nax4.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nax4.set_title(f'XGBoost: Val RMSE={xgb_rmse:.4f}', fontsize=11, fontweight='bold')\nax4.set_xlabel('Actual')\nax4.set_ylabel('Predicted')\n\n# 5. Validation Set Performance - LightGBM\nax5 = fig.add_subplot(gs[1, 1])\nax5.scatter(y_val, lgb_pred_val, alpha=0.5, s=10, color='orange')\nax5.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nax5.set_title(f'LightGBM: Val RMSE={lgb_rmse:.4f}', fontsize=11, fontweight='bold')\nax5.set_xlabel('Actual')\nax5.set_ylabel('Predicted')\n\n# 6. Validation Set Performance - Ensemble\nax6 = fig.add_subplot(gs[1, 2])\nensemble_val = (weights[0] * xgb_pred_val + weights[1] * lgb_pred_val +\n                 weights[2] * cat_pred_val + weights[3] * rf_pred_val)\nensemble_rmse_val = np.sqrt(mean_squared_error(y_val, ensemble_val))\nax6.scatter(y_val, ensemble_val, alpha=0.5, s=10, color='green')\nax6.plot([y_val.min(), y_val.max()], [y_val.min(), y_val.max()], 'r--', lw=2)\nax6.set_title(f'Ensemble: Val RMSE={ensemble_rmse_val:.4f}', fontsize=11, fontweight='bold')\nax6.set_xlabel('Actual')\nax6.set_ylabel('Predicted')\n\n# 7. Cross-Validation Score Distribution\nax7 = fig.add_subplot(gs[2, 0])\nax7.boxplot([xgb_cv_rmse, lgb_cv_rmse, cat_cv_rmse, rf_cv_rmse],\n             labels=models)\nax7.set_title('Cross-Validation RMSE Distribution', fontsize=11, fontweight='bold')\nax7.set_ylabel('RMSE')\nax7.tick_params(axis='x', rotation=45)\n\n# 8. Model Predictions Comparison\nax8 = fig.add_subplot(gs[2, 1])\nax8.hist(xgb_pred_test, bins=40, alpha=0.5, label='XGBoost', edgecolor='black')\nax8.hist(lgb_pred_test, bins=40, alpha=0.5, label='LightGBM', edgecolor='black')\nax8.hist(ensemble_pred, bins=40, alpha=0.5, label='Ensemble', edgecolor='black')\nax8.set_title('Prediction Distribution Comparison', fontsize=11, fontweight='bold')\nax8.set_xlabel('Accident Risk')\nax8.set_ylabel('Frequency')\nax8.legend()\n\n# 9. Summary Statistics Table (as text)\nax9 = fig.add_subplot(gs[2, 2])\nax9.axis('off')\nsummary_text = f\"\"\"SUMMARY STATISTICS\n\nValidation RMSE:\n  XGB: {xgb_rmse:.6f}\n  LGB: {lgb_rmse:.6f}\n  CAT: {cat_rmse:.6f}\n  RF:  {rf_rmse:.6f}\n\nCV RMSE (Mean±Std):\n  XGB: {xgb_cv_rmse.mean():.4f}±{xgb_cv_rmse.std():.4f}\n  LGB: {lgb_cv_rmse.mean():.4f}±{lgb_cv_rmse.std():.4f}\n\nEnsemble R²: {r2_score(y_val, ensemble_val):.4f}\n\"\"\"\nax9.text(0.1, 0.5, summary_text, fontsize=10, family='monospace',\n         verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n\nplt.savefig('model_performance.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint('Performance visualization saved!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T22:20:15.751616Z","iopub.execute_input":"2025-10-31T22:20:15.752084Z","iopub.status.idle":"2025-10-31T22:20:22.340595Z","shell.execute_reply.started":"2025-10-31T22:20:15.752049Z","shell.execute_reply":"2025-10-31T22:20:22.339705Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feature Importance Analysis","metadata":{}},{"cell_type":"code","source":"# Feature Importance Analysis - CORRECTED VERSION\n\n# Get feature names from the original data\nfeature_names = []\n\n# Add numerical feature names\nnumerical_features_list = X_train.select_dtypes(include=['int64', 'float64']).columns.tolist()\nfeature_names.extend(numerical_features_list)\n\n# Add categorical feature names\ncategorical_features_list = X_train.select_dtypes(include=['object']).columns.tolist()\nfeature_names.extend(categorical_features_list)\n\nprint(f'Total features in X_train: {len(X_train.columns)}')\nprint(f'Feature names created: {len(feature_names)}')\n\n# Now let's get feature importances\nfig, axes = plt.subplots(2, 2, figsize=(16, 10))\n\n# ===== XGBoost Feature Importance =====\ntry:\n    xgb_importances = xgb_final.feature_importances_\n    if len(xgb_importances) == len(feature_names):\n        xgb_imp = pd.DataFrame({\n            'feature': feature_names,\n            'importance': xgb_importances\n        }).sort_values('importance', ascending=False).head(15)\n    else:\n        # If lengths don't match, create generic feature names\n        xgb_imp = pd.DataFrame({\n            'feature': [f'Feature_{i}' for i in range(len(xgb_importances))],\n            'importance': xgb_importances\n        }).sort_values('importance', ascending=False).head(15)\n    \n    axes[0, 0].barh(range(len(xgb_imp)), xgb_imp['importance'], color='#FF6B6B')\n    axes[0, 0].set_yticks(range(len(xgb_imp)))\n    axes[0, 0].set_yticklabels(xgb_imp['feature'])\n    axes[0, 0].set_title('XGBoost Feature Importance', fontsize=12, fontweight='bold')\n    axes[0, 0].invert_yaxis()\nexcept Exception as e:\n    print(f\"Error with XGBoost: {e}\")\n\n# ===== LightGBM Feature Importance =====\ntry:\n    lgb_importances = lgb_final.feature_importances_\n    if len(lgb_importances) == len(feature_names):\n        lgb_imp = pd.DataFrame({\n            'feature': feature_names,\n            'importance': lgb_importances\n        }).sort_values('importance', ascending=False).head(15)\n    else:\n        lgb_imp = pd.DataFrame({\n            'feature': [f'Feature_{i}' for i in range(len(lgb_importances))],\n            'importance': lgb_importances\n        }).sort_values('importance', ascending=False).head(15)\n    \n    axes[0, 1].barh(range(len(lgb_imp)), lgb_imp['importance'], color='#4ECDC4')\n    axes[0, 1].set_yticks(range(len(lgb_imp)))\n    axes[0, 1].set_yticklabels(lgb_imp['feature'])\n    axes[0, 1].set_title('LightGBM Feature Importance', fontsize=12, fontweight='bold')\n    axes[0, 1].invert_yaxis()\nexcept Exception as e:\n    print(f\"Error with LightGBM: {e}\")\n\n# ===== CatBoost Feature Importance =====\ntry:\n    cat_importances = cat_final.get_feature_importance()\n    if len(cat_importances) == len(feature_names):\n        cat_imp = pd.DataFrame({\n            'feature': feature_names,\n            'importance': cat_importances\n        }).sort_values('importance', ascending=False).head(15)\n    else:\n        cat_imp = pd.DataFrame({\n            'feature': [f'Feature_{i}' for i in range(len(cat_importances))],\n            'importance': cat_importances\n        }).sort_values('importance', ascending=False).head(15)\n    \n    axes[1, 0].barh(range(len(cat_imp)), cat_imp['importance'], color='#45B7D1')\n    axes[1, 0].set_yticks(range(len(cat_imp)))\n    axes[1, 0].set_yticklabels(cat_imp['feature'])\n    axes[1, 0].set_title('CatBoost Feature Importance', fontsize=12, fontweight='bold')\n    axes[1, 0].invert_yaxis()\nexcept Exception as e:\n    print(f\"Error with CatBoost: {e}\")\n\n# ===== Random Forest Feature Importance =====\ntry:\n    rf_importances = rf_final.feature_importances_\n    if len(rf_importances) == len(feature_names):\n        rf_imp = pd.DataFrame({\n            'feature': feature_names,\n            'importance': rf_importances\n        }).sort_values('importance', ascending=False).head(15)\n    else:\n        rf_imp = pd.DataFrame({\n            'feature': [f'Feature_{i}' for i in range(len(rf_importances))],\n            'importance': rf_importances\n        }).sort_values('importance', ascending=False).head(15)\n    \n    axes[1, 1].barh(range(len(rf_imp)), rf_imp['importance'], color='#96CEB4')\n    axes[1, 1].set_yticks(range(len(rf_imp)))\n    axes[1, 1].set_yticklabels(rf_imp['feature'])\n    axes[1, 1].set_title('Random Forest Feature Importance', fontsize=12, fontweight='bold')\n    axes[1, 1].invert_yaxis()\nexcept Exception as e:\n    print(f\"Error with Random Forest: {e}\")\n\nplt.tight_layout()\nplt.savefig('feature_importance.png', dpi=300, bbox_inches='tight')\nplt.show()\nprint('Feature importance visualization saved!')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T22:20:22.342093Z","iopub.execute_input":"2025-10-31T22:20:22.34237Z","iopub.status.idle":"2025-10-31T22:20:25.820271Z","shell.execute_reply.started":"2025-10-31T22:20:22.342348Z","shell.execute_reply":"2025-10-31T22:20:25.819217Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Final Summary Report","metadata":{}},{"cell_type":"code","source":"# Final comprehensive summary\nprint('\\n' + '='*70)\nprint('FINAL MODEL PERFORMANCE SUMMARY'.center(70))\nprint('='*70)\n\nprint('\\n1. INDIVIDUAL MODEL PERFORMANCE:')\nprint('-'*70)\nprint(f'\\nValidation Set Metrics:')\nprint(f'  Model           RMSE      MAE       R² Score')\nprint(f'  {\"─\"*60}')\nprint(f'  XGBoost:        {xgb_rmse:.6f}  {xgb_mae:.6f}  {xgb_r2:.6f}')\nprint(f'  LightGBM:       {lgb_rmse:.6f}  {lgb_mae:.6f}  {lgb_r2:.6f}')\nprint(f'  CatBoost:       {cat_rmse:.6f}  {cat_mae:.6f}  {cat_r2:.6f}')\nprint(f'  Random Forest:  {rf_rmse:.6f}  {rf_mae:.6f}  {rf_r2:.6f}')\n\nprint('\\n\\n2. CROSS-VALIDATION PERFORMANCE (5-Fold):')\nprint('-'*70)\nprint(f'  Model           Mean RMSE    Std Dev     Min         Max')\nprint(f'  {\"─\"*60}')\nprint(f'  XGBoost:        {xgb_cv_rmse.mean():.6f}      {xgb_cv_rmse.std():.6f}      {xgb_cv_rmse.min():.6f}      {xgb_cv_rmse.max():.6f}')\nprint(f'  LightGBM:       {lgb_cv_rmse.mean():.6f}      {lgb_cv_rmse.std():.6f}      {lgb_cv_rmse.min():.6f}      {lgb_cv_rmse.max():.6f}')\nprint(f'  CatBoost:       {cat_cv_rmse.mean():.6f}      {cat_cv_rmse.std():.6f}      {cat_cv_rmse.min():.6f}      {cat_cv_rmse.max():.6f}')\nprint(f'  Random Forest:  {rf_cv_rmse.mean():.6f}      {rf_cv_rmse.std():.6f}      {rf_cv_rmse.min():.6f}      {rf_cv_rmse.max():.6f}')\n\nprint('\\n\\n3. ENSEMBLE MODEL CONFIGURATION:')\nprint('-'*70)\nprint(f'  Model           Weight      Percentage')\nprint(f'  {\"─\"*60}')\nprint(f'  XGBoost:        {weights[0]:.6f}      {weights[0]*100:.2f}%')\nprint(f'  LightGBM:       {weights[1]:.6f}      {weights[1]*100:.2f}%')\nprint(f'  CatBoost:       {weights[2]:.6f}      {weights[2]*100:.2f}%')\nprint(f'  Random Forest:  {weights[3]:.6f}      {weights[3]*100:.2f}%')\n\nprint('\\n\\n4. TEST SET PREDICTIONS SUMMARY:')\nprint('-'*70)\nprint(f'  Statistic                   Value')\nprint(f'  {\"─\"*60}')\nprint(f'  Number of predictions:     {len(ensemble_pred):>10}')\nprint(f'  Mean prediction:           {ensemble_pred.mean():>10.6f}')\nprint(f'  Std deviation:             {ensemble_pred.std():>10.6f}')\nprint(f'  Min prediction:            {ensemble_pred.min():>10.6f}')\nprint(f'  Max prediction:            {ensemble_pred.max():>10.6f}')\nprint(f'  Median prediction:         {np.median(ensemble_pred):>10.6f}')\n\nprint('\\n\\n5. OUTPUT FILES GENERATED:')\nprint('-'*70)\nprint('  ✓ submission.csv')\nprint('  ✓ feature_importance.png')\n\nprint('\\n' + '='*70)\nprint('ANALYSIS COMPLETE'.center(70))\nprint('='*70 + '\\n')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-10-31T22:20:25.821361Z","iopub.execute_input":"2025-10-31T22:20:25.8217Z","iopub.status.idle":"2025-10-31T22:20:25.842539Z","shell.execute_reply.started":"2025-10-31T22:20:25.821678Z","shell.execute_reply":"2025-10-31T22:20:25.841768Z"}},"outputs":[],"execution_count":null}]}